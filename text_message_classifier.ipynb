{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam text message classifier\n",
    "This interactive notebook will explore text message data found on [Kaggle.com](https://www.kaggle.com/team-ai/spam-text-message-classification) and create models to predict whether or not the message is spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "Import the data from `SPAM text message 20170820 - Data.csv`. The file contains a label `Category`, which is whether or not the transaction is spam. Each row corresponds to a text message. \n",
    "\n",
    "Here we'll import the data into a Pandas dataframe and change the `spam` instances in `Category` to a 1, and replace other instances (the `ham` messages) with 0s. Then we'll compute the percentage of messages that are spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('SPAM text message 20170820 - Data.csv')\n",
    "\n",
    "spam_data['Category'] = np.where(spam_data['Category']=='spam',1,0)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['Message'], \n",
    "                                                    spam_data['Category'], \n",
    "                                                    random_state=0)\n",
    "\n",
    "spam_data['Category'].mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "First, we'll fit the training data X_train using Count Vectorizer with default parameters.\n",
    "For fun, we print the longest token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hypotheticalhuagauahahuagahyuhagga'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer().fit(X_train)\n",
    "max(vec.get_feature_names(),key = lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "Now we'll fit and transform the training data X_train using a Count Vectorizer with \n",
    "default parameters. Next, we'll fit a multinomial Naive Bayes classifier model with smoothing\n",
    "alpha = 0.1. We compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619675138714874"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train_vectorized = vec.transform(X_train)\n",
    "\n",
    "model = MultinomialNB(alpha = 0.1)\n",
    "model.fit(X_train_vectorized,y_train)\n",
    "\n",
    "predictions = model.predict(vec.transform(X_test))\n",
    "roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf vectorization\n",
    "Now we'll fit and transform the training data X_train using a Tfidf Vectorizer\n",
    "with default parameters. To get an idea of how the Tfidf Vectorizer differs from the count vectorizer, we'll return a series containing 20 features that have the smallest tf-idf and 20 features\n",
    "having the largest tf-idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exterminator    0.075064\n",
      "psychiatrist    0.075064\n",
      "psychologist    0.075064\n",
      "pudunga         0.075064\n",
      "chef            0.075064\n",
      "sympathetic     0.075064\n",
      "healer          0.075064\n",
      "aaniye          0.075064\n",
      "venaam          0.075064\n",
      "companion       0.075064\n",
      "organizer       0.075064\n",
      "courageous      0.075064\n",
      "athletic        0.075064\n",
      "determined      0.075064\n",
      "listener        0.075064\n",
      "pest            0.075064\n",
      "dependable      0.075064\n",
      "stylist         0.075064\n",
      "dasara          0.092161\n",
      "sankranti       0.092161\n",
      "dtype: float64 \n",
      " ok                1.000000\n",
      "nite              1.000000\n",
      "too               1.000000\n",
      "alrite            1.000000\n",
      "lei               1.000000\n",
      "yup               1.000000\n",
      "error             1.000000\n",
      "anything          1.000000\n",
      "146tf150p         1.000000\n",
      "645               1.000000\n",
      "anytime           1.000000\n",
      "home              1.000000\n",
      "thanx             1.000000\n",
      "done              1.000000\n",
      "thank             1.000000\n",
      "congratulation    1.000000\n",
      "where             1.000000\n",
      "beerage           1.000000\n",
      "okie              1.000000\n",
      "hi                0.978631\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer().fit(X_train)\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "smallest_tfidfs = pd.Series(X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[:20]],\n",
    "index = feature_names[sorted_tfidf_index[:20]])\n",
    "largest_tfidfs = pd.Series(X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[:-21:-1]],\n",
    "index = feature_names[sorted_tfidf_index[:-21:-1]])\n",
    "print(smallest_tfidfs,'\\n',largest_tfidfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a \n",
    "document frequency strictly lower than 3.\n",
    "\n",
    "Then we'll fit a multinomial Naive Bayes classifier model with smoothing alpha=0.1 and compute the area under\n",
    "the curve (AUC) score using the transformed test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9536401467692859"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df= 3).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = MultinomialNB(alpha = 0.1)\n",
    "model.fit(X_train_vectorized,y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving our model by adding features\n",
    "In order to make our TFIDF model better, we will add features. To this end, we define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start to find features to add. For instance, we average length of an authentic message vs the average length of a spam message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.9892904953146\n",
      "71.44829015544042\n"
     ]
    }
   ],
   "source": [
    "avg_spam_len = np.mean(list(map(len,spam_data[spam_data['Category']==1]['Message'])))\n",
    "avg_ham_len = np.mean(list(map(len,spam_data[spam_data['Category']==0]['Message'])))\n",
    "\n",
    "print(avg_spam_len)\n",
    "print(avg_ham_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the average spam message is twice as long as the average authentic message. This motivates fitting and training the data X_train using a Tfidf Vectorizer and ignoring terms that have\n",
    "a document frequency strictly lower than 5. Then we'll use this document-term matrix and an additional feature, the length of the document (number of \n",
    "characters), to fit a Support Vector Classifier model with regularization C=10000. We compute\n",
    "the area under hte curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956562108466082"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "vect = TfidfVectorizer(min_df= 5).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "doc_length = np.array(list(map(len, X_train)))\n",
    "\n",
    "X_train_vectorized = add_feature(X_train_vectorized , doc_length)\n",
    "\n",
    "model = SVC(C=10000, gamma = 'auto').fit(X_train_vectorized,y_train)\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "test_doc_length = np.array(list(map(len,X_test)))\n",
    "\n",
    "X_test_vectorized = add_feature(X_test_vectorized, test_doc_length)\n",
    "\n",
    "predictions = model.predict(X_test_vectorized)\n",
    "roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at the average number of digits per message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30528497409326427\n",
      "15.639892904953147\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "digits_non_spam = spam_data[spam_data['Category']==0]['Message'].apply(lambda x: ''.join(re.findall(r'[0-9]+',x)))\n",
    "avg_digits_ham = np.mean(list(map(len, digits_non_spam)))\n",
    "\n",
    "digits_spam = spam_data[spam_data['Category']==1]['Message'].apply(lambda x: ''.join(re.findall(r'[0-9]+',x)))\n",
    "avg_digits_spam = np.mean(list(map(len, digits_spam)))\n",
    "\n",
    "print(avg_digits_ham)\n",
    "print(avg_digits_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam messages are far more likely to contain digits when compared to authentic messages. With this in mind, we'll fit and transform the training data X_train using Tfidf Vectorizer ignoring terms that have\n",
    "a document frequency strictly lower than 5 and using words n-grams from n=1 to n =3. Using this\n",
    "document-term matrix and the following additional features: (1) the length of document (number\n",
    "of characters) (2) number of digits per document, we'll fit a Logistic Regression model with regularization\n",
    "C=100. Then compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.966325845713263"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vect = TfidfVectorizer(min_df= 5, ngram_range = (1,3) ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "doc_length = np.array(list(map(len, X_train)))\n",
    "\n",
    "X_train_vectorized = add_feature(X_train_vectorized , doc_length)\n",
    "\n",
    "doc_num_of_digits = np.array(list(map(len, X_train.apply( lambda x: ''.join( re.findall( r'[0-9]+',x ) ) ) ) ) )\n",
    "X_train_vectorized = add_feature( X_train_vectorized, doc_num_of_digits )\n",
    "\n",
    "model = LogisticRegression(C=100, solver = 'liblinear')\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "test_doc_length = np.array(list(map(len,X_test)))\n",
    "X_test_vectorized = add_feature(X_test_vectorized, test_doc_length)\n",
    "\n",
    "test_doc_num_of_digits = np.array(list(map(len, X_test.apply( lambda x: ''.join( re.findall( r'[0-9]+',x ) ) ) ) ) )\n",
    "X_test_vectorized = add_feature(X_test_vectorized, test_doc_num_of_digits)\n",
    "\n",
    "predictions = model.predict(X_test_vectorized)\n",
    "roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at the average number of non-word characters (anything other than a letter, digit or \n",
    "underscore) per document for ham and spam documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.362072538860104\n",
      "28.9384203480589\n"
     ]
    }
   ],
   "source": [
    "non_word_chars_non_spam = spam_data[spam_data['Category']==0]['Message'].apply(lambda x: ''.join(re.findall(r'\\W+',x)))\n",
    "ham_non_word_avg = np.mean(list(map(len, non_word_chars_non_spam)))\n",
    "\n",
    "non_word_chars_spam = spam_data[spam_data['Category']==1]['Message'].apply(lambda x: ''.join(re.findall(r'\\W+',x)))\n",
    "spam_non_word_avg = np.mean(list(map(len, non_word_chars_spam)))\n",
    "\n",
    "print(ham_non_word_avg)\n",
    "print(spam_non_word_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll put all these features together. That is, we'll fit and transform the training data X_train using a Count Vectorizer ignoring terms that \n",
    "have a document frequency stricly lower than 5 and using character n-grams n=2 to n=5.\n",
    "\n",
    "Note: passing analyzer='char_wb' to the Count Vectorizer will create\n",
    "character n-grams only from text inside word boundaries. This should make the model more immune to spelling mistakes. Using this document-term matrix and the following additional\n",
    "features: (1) the length of document (number of characters), (2) number of digits per doc and\n",
    "(3) number of non-word characters (anything other than a letter, digit of underscore), we'll fit\n",
    "a Logistic Regression model with regularization C=100. Then compute the area under the curve\n",
    "(AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9721451584034366"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df= 5, ngram_range = (2,5), analyzer = 'char_wb' ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "doc_length = np.array(list(map(len, X_train)))\n",
    "\n",
    "X_train_vectorized = add_feature(X_train_vectorized , doc_length)\n",
    "\n",
    "doc_num_of_digits = np.array(list(map(len, X_train.apply( lambda x: ''.join( re.findall( r'[0-9]+',x ) ) ) ) ) )\n",
    "X_train_vectorized = add_feature( X_train_vectorized, doc_num_of_digits )\n",
    "\n",
    "doc_non_word_chars = np.array(list(map(len, X_train.apply( lambda x: ''.join(re.findall(r'\\W+',x)) ))))\n",
    "X_train_vectorized = add_feature( X_train_vectorized, doc_non_word_chars )\n",
    "\n",
    "model = LogisticRegression(C=100, solver = 'liblinear')\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "test_doc_length = np.array(list(map(len,X_test)))\n",
    "X_test_vectorized = add_feature(X_test_vectorized, test_doc_length)\n",
    "\n",
    "\n",
    "test_doc_num_of_digits = np.array(list(map(len, X_test.apply( lambda x: ''.join( re.findall( r'[0-9]+',x ) ) ) ) ) )\n",
    "X_test_vectorized = add_feature(X_test_vectorized, test_doc_num_of_digits)\n",
    "\n",
    "test_doc_non_word_chars = np.array(list(map(len, X_test.apply( lambda x: ''.join(re.findall(r'\\W+',x)) ))))\n",
    "X_test_vectorized = add_feature( X_test_vectorized, test_doc_non_word_chars )\n",
    "\n",
    "predictions = model.predict(X_test_vectorized)\n",
    "roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these new features paid off! However, it would be nice to see specifically which features are the most important features for this classifier. Since we trained a Logistic Regression model, this can be achieved by looking at the coefficients in the classifier. We'll look at the ten smallest and ten largest weights to see what are some of the most important features, according to our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..    -1.039249\n",
      " m    -0.754072\n",
      " y    -0.703007\n",
      "?     -0.695312\n",
      ".     -0.690813\n",
      " i    -0.625795\n",
      "go    -0.607371\n",
      ":)    -0.600318\n",
      "h     -0.569786\n",
      " go   -0.547986\n",
      "dtype: float64\n",
      "digit_count    1.188104\n",
      "r!             1.016638\n",
      " #             1.016613\n",
      "r!             0.970923\n",
      "erro           0.912133\n",
      "error          0.912133\n",
      "rror           0.906075\n",
      "ror            0.906075\n",
      "or             0.851032\n",
      "err            0.811515\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "feature_names = np.append(feature_names,['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "smallest_coefs = pd.Series(model.coef_[0][sorted_coef_index[:10]],\n",
    "index = feature_names[sorted_coef_index[:10]])\n",
    "largest_coefs = pd.Series(model.coef_[0][sorted_coef_index[:-11:-1]],\n",
    "index = feature_names[sorted_coef_index[:-11:-1]])\n",
    "\n",
    "print(smallest_coefs)\n",
    "print(largest_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, from the smaller weights (meaning the corresponding features are useful for identifying ham messages) we see some familiar text message features, e.g. a smiley emoticon, part of an ellipsis, 'go' or 'i'. From the larger weights (meaning the corresponding features are useful for indentifying spam messages) we see 'error', some symbols, and the most important feature: the number of digits contained in message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "python-data-analysis",
   "graded_item_id": "zAr06",
   "launcher_item_id": "KSSjT",
   "part_id": "SL3fU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
